{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6884e39c",
   "metadata": {},
   "source": [
    " notebook provides a production-ready pipeline for:\n",
    "\n",
    "1. Converting COCO axis-aligned bboxes to oriented bounding boxes (OBBs).\n",
    "2. Cropping image patches based on OBBs and trimming borders.\n",
    "3. Predicting object orientation using a trained ResNet18 model.\n",
    "4. Updating COCO annotations with predicted rotations.\n",
    "5. Running end-to-end over all batches in a specified directory.\n",
    "\n",
    "The flow follows modular functions, dynamic batch discovery, progress bars, and clean logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07797539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rotation_pipeline:Loading model from /Users/gerhardkarbeutz/cerpro/ocr-rec-lab/pipeline/checkpoints/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rotation_pipeline:Loading COCO JSON: /Users/gerhardkarbeutz/cerpro/ocr-rec-lab/data/rotation/batches/rotation_20250721_01/annotations/instances_default.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d5f3ec87dd4cc6b4667f0cd7e2ef55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rotation_20250721_01:   0%|          | 0/21163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>0.0</th>\n",
       "      <th>90.0</th>\n",
       "      <th>180.0</th>\n",
       "      <th>270.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch</th>\n",
       "      <th>orig</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">rotation_20250721_01</th>\n",
       "      <th>0.0</th>\n",
       "      <td>13062</td>\n",
       "      <td>37</td>\n",
       "      <td>165</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90.0</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180.0</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270.0</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred                        0.0    90.0   180.0  270.0\n",
       "batch                orig                             \n",
       "rotation_20250721_01 0.0    13062     37    165   1180\n",
       "                     90.0      69      0      9     26\n",
       "                     180.0     38      0      2      4\n",
       "                     270.0    890      1     46    452"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "# Imports & Configuration\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Base directory (adjust if needed)\n",
    "BASE_DIR = Path().resolve().parent\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"rotation\" / \"batches\"\n",
    "CHECKPOINT_PATH = BASE_DIR / \"pipeline\" / \"checkpoints\" / \"best_model.pth\"\n",
    "DEBUG_IMAGES_DIR = BASE_DIR / \"pipeline\" / \"debug_imgs\"\n",
    "RESULTS_CSV = BASE_DIR / \"pipeline\" / \"results.csv\"\n",
    "\n",
    "# Classes and device\n",
    "CLASS_NAMES = [0, 90, 180, 270]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = 300\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"rotation_pipeline\")\n",
    "\n",
    "# Transform\n",
    "from torchvision.transforms import InterpolationMode\n",
    "TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# %%\n",
    "# I/O Functions\n",
    "def load_coco(path: Path) -> dict:\n",
    "    logger.info(f\"Loading COCO JSON: {path}\")\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def save_coco(coco: dict, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Saving updated COCO to {path}\")\n",
    "    path.write_text(json.dumps(coco, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# %%\n",
    "# Model Loading\n",
    "def load_model(ckpt_path: Path) -> nn.Module:\n",
    "    logger.info(f\"Loading model from {ckpt_path}\")\n",
    "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    model.fc = nn.Linear(model.fc.in_features, len(CLASS_NAMES))\n",
    "    ckpt = torch.load(str(ckpt_path), map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    return model.to(DEVICE).eval()\n",
    "\n",
    "# %%\n",
    "# OBB Utilities\n",
    "def create_obb(ann: Dict[str, Any]):\n",
    "    x, y, w, h = ann[\"bbox\"]\n",
    "    cx, cy = x + w/2, y + h/2\n",
    "    angle = ann.get(\"attributes\", {}).get(\"rotation\", 0.0)\n",
    "    ann[\"bbox\"] = [cx, cy, w, h, angle]\n",
    "\n",
    "# %%\n",
    "# Image & Rotation Helpers\n",
    "def crop_obb_trim(img: np.ndarray, cx, cy, w, h, angle, pad=0) -> np.ndarray:\n",
    "    theta = np.deg2rad(angle)\n",
    "    R = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]], np.float32)\n",
    "    corners = np.float32([[-w/2, -h/2], [w/2, -h/2], [w/2, h/2], [-w/2, h/2]]) @ R.T + np.array([cx, cy])\n",
    "    xs, ys = corners[:,0], corners[:,1]\n",
    "    x0, x1 = max(int(np.floor(xs.min()))-pad, 0), min(int(np.ceil(xs.max()))+pad, img.shape[1]-1)\n",
    "    y0, y1 = max(int(np.floor(ys.min()))-pad, 0), min(int(np.ceil(ys.max()))+pad, img.shape[0]-1)\n",
    "    roi = img[y0:y1+1, x0:x1+1]\n",
    "    mask = cv2.fillPoly(np.zeros(roi.shape[:2], np.uint8), [np.round(corners - [x0, y0]).astype(np.int32)], 255)\n",
    "    masked = cv2.bitwise_and(roi, roi, mask=mask)\n",
    "    ys_nz, xs_nz = np.where(mask>0)\n",
    "    return masked[ys_nz.min():ys_nz.max()+1, xs_nz.min():xs_nz.max()+1]\n",
    "\n",
    "def predict_rotation(model: nn.Module, patch: np.ndarray) -> float:\n",
    "    rgb = cv2.cvtColor(patch, cv2.COLOR_BGR2RGB)\n",
    "    tensor = TRANSFORM(Image.fromarray(rgb)).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)\n",
    "    return float(CLASS_NAMES[torch.argmax(logits, dim=1).item()])\n",
    "\n",
    "# %%\n",
    "# Batch Processing\n",
    "def process_batch(batch_dir: Path, model: nn.Module, debug: bool=False) -> pd.DataFrame:\n",
    "    img_dir = batch_dir / \"images\" / \"default\"\n",
    "    coco_default = load_coco(batch_dir / \"annotations\" / \"instances_default.json\")\n",
    "    coco_obb = deepcopy(coco_default)\n",
    "    for ann in coco_obb[\"annotations\"]:\n",
    "        create_obb(ann)\n",
    "\n",
    "    records = []\n",
    "    cache = {}\n",
    "    for ann in tqdm(coco_obb[\"annotations\"], desc=batch_dir.name):\n",
    "        cx, cy, w, h, orig = ann[\"bbox\"]\n",
    "        if orig not in CLASS_NAMES:\n",
    "            continue\n",
    "        img_info = next(img for img in coco_default[\"images\"] if img[\"id\"]==ann[\"image_id\"])\n",
    "        path = img_dir / img_info[\"file_name\"]\n",
    "        if not path.exists():\n",
    "            logger.error(f\"Missing file {path}\")\n",
    "            continue\n",
    "        # Load image into cache\n",
    "        if path in cache:\n",
    "            img = cache[path]\n",
    "        else:\n",
    "            img = cv2.imread(str(path))\n",
    "            if img is None:\n",
    "                logger.error(f\"Failed to read {path}\")\n",
    "                continue\n",
    "            cache[path] = img\n",
    "\n",
    "        patch = crop_obb_trim(img, cx, cy, w, h, orig)\n",
    "        pred = predict_rotation(model, patch)\n",
    "        records.append({\"id\": ann[\"id\"], \"orig\": orig, \"pred\": pred, \"file\": path.name})\n",
    "        if debug:\n",
    "            DEBUG_IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "            cv2.imwrite(str(DEBUG_IMAGES_DIR/ f\"{batch_dir.name}_{ann['id']}.png\"), patch)\n",
    "    from pandas import DataFrame\n",
    "    return DataFrame(records)\n",
    "\n",
    "# %%\n",
    "# Run All Batches\n",
    "\n",
    "torch_model = load_model(CHECKPOINT_PATH)\n",
    "all_results = []\n",
    "for batch in sorted(DATA_DIR.iterdir()):\n",
    "    if not batch.is_dir() or \"rotation\" not in batch.name:\n",
    "        continue\n",
    "    df = process_batch(batch, torch_model, debug=False)\n",
    "    df[\"batch\"] = batch.name\n",
    "    all_results.append(df)\n",
    "\n",
    "if all_results:\n",
    "    df_all = pd.concat(all_results, ignore_index=True)\n",
    "    display(df_all.groupby([\"batch\",\"orig\"])['pred']\n",
    "            .value_counts().unstack(fill_value=0))\n",
    "    df_all.to_csv(RESULTS_CSV, index=False)\n",
    "else:\n",
    "    logger.warning(\"No batches processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr-rec-lab-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
